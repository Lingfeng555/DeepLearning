{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Literal\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "SEED = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    \n",
    "    users: dict\n",
    "    ratings: pd.DataFrame\n",
    "    users_min_number_of_ratings: pd.DataFrame\n",
    "    split : str\n",
    "    def __init__(self, ml_path: str, split: Literal[\"train\", \"val\", \"test\"], seed: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "        data = pd.read_csv(os.path.join(ml_path,\"u.data\"),\n",
    "                 sep='\\t',    \n",
    "                 header=None,  \n",
    "                 names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "        data['timestamp'] = (data['timestamp'] - pd.Timestamp(\"1970-01-01\").second )/(3600*24*365)\n",
    "        \n",
    "        genre = pd.read_csv(os.path.join(ml_path,\"u.genre\"),\n",
    "                 sep='|',    \n",
    "                 header=None,  \n",
    "                 names=['genre_name', 'genre_id'])\n",
    "        \n",
    "        columns = [\n",
    "            'movie_id',\n",
    "            'movie_title',\n",
    "            'release_date',\n",
    "            'video_release_date',\n",
    "            'IMDb_URL'\n",
    "        ] + genre[\"genre_name\"].tolist()\n",
    "        \n",
    "        \n",
    "        movie = pd.read_csv(\n",
    "            os.path.join(ml_path,\"u.item\"),\n",
    "            sep='|',\n",
    "            header=None,        \n",
    "            names=columns,     \n",
    "            encoding='latin-1'  \n",
    "        ).drop(columns=['video_release_date', 'IMDb_URL'])\n",
    "        \n",
    "        movie[\"release_date\"] = pd.to_datetime(movie[\"release_date\"], errors='coerce') - pd.Timestamp(\"1970-01-01\")\n",
    "        movie['years_1970'] = movie['release_date'].dt.days / 365\n",
    "        movie = movie.drop(columns=['release_date', \"movie_title\"])\n",
    "        \n",
    "        users = pd.read_csv(os.path.join(ml_path,\"u.user\"),\n",
    "                            sep='|',\n",
    "                            header=None,        \n",
    "                            names= \"user id | age | gender | occupation | zip code\".split(\" | \"),     \n",
    "                            encoding='latin-1')\n",
    "        \n",
    "        users[\"gender\"] = users[\"gender\"].apply(lambda x: 1 if x == \"M\" else 0)\n",
    "        users[\"occupation\"] = users[\"occupation\"]\n",
    "        users[\"occupation\"] = pd.Categorical(users[\"occupation\"])\n",
    "        occupation_dummies = pd.get_dummies(users['occupation'], prefix='occupation').astype(int)\n",
    "        users = pd.concat([users, occupation_dummies], axis=1).drop(columns=[\"occupation\", \"zip code\"])\n",
    "        \n",
    "        ratings = pd.merge(\n",
    "            data,           \n",
    "            movie,          \n",
    "            how='left',     \n",
    "            left_on='item_id',\n",
    "            right_on='movie_id'\n",
    "        )\n",
    "        ratings.head()\n",
    "        ratings.drop(columns=[\"item_id\", \"movie_id\"], inplace=True)\n",
    "        ratings[\"years_since_review\"] = ratings[\"timestamp\"] - ratings[\"years_1970\"]\n",
    "        ratings.drop(columns=[\"years_1970\"], inplace=True)\n",
    "        \n",
    "        self.ratings = ratings\n",
    "\n",
    "        self.users_min_number_of_ratings = ratings[\"user_id\"].value_counts().min() - 1\n",
    "        \n",
    "        users_train_val, users_test = train_test_split(\n",
    "            users, \n",
    "            test_size=0.2, \n",
    "            random_state=seed\n",
    "        )\n",
    "\n",
    "        users_train, users_val = train_test_split(\n",
    "            users_train_val,\n",
    "            test_size=0.1,\n",
    "            random_state=seed\n",
    "        )\n",
    "        self.users = {}\n",
    "        self.users[\"val\"] = users_val\n",
    "        self.users[\"test\"] = users_test\n",
    "        self.users[\"train\"] = users_train\n",
    "        \n",
    "        self.split = split\n",
    "        \n",
    "    def __len__(self): return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[self.split].iloc[index].astype(int)\n",
    "        user_id = user[\"user id\"]\n",
    "        user_data = user.drop(\"user id\")\n",
    "        \n",
    "        rating_hist = self.ratings[self.ratings[\"user_id\"] == user_id].drop(columns=[\"user_id\"]).sort_values(by='timestamp', ascending=True)\n",
    "        rating_hist[\"timestamp\"] = self.scaler.fit_transform(rating_hist[[\"timestamp\"]])\n",
    "        rating_hist[\"years_since_review\"] = self.scaler.fit_transform(rating_hist[[\"years_since_review\"]])\n",
    "        rating_hist[\"rating\"] = self.scaler.fit_transform(rating_hist[[\"rating\"]])\n",
    "        \n",
    "        user_data__tensor = torch.tensor(user_data.values)\n",
    "        rating_hist_tensor = torch.tensor(rating_hist.values)\n",
    "        return user_data__tensor, rating_hist_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MovieLensDataset(ml_path=\"ml-100k\", split=\"test\", seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0]),\n",
       " tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00, 0.0000e+00,\n",
       "          1.2016e-02],\n",
       "         [7.5000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          1.0435e-02],\n",
       "         [7.5000e-01, 1.3778e-04, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          1.7391e-02],\n",
       "         ...,\n",
       "         [2.5000e-01, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          2.1503e-02],\n",
       "         [5.0000e-01, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          7.6519e-02],\n",
       "         [7.5000e-01, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          6.7045e-01]], dtype=torch.float64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
