{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Literal\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from utils.loader import MovieLensDataset\n",
    "from arquitecture.Recommender import Recommender\n",
    "from arquitecture.components.PatterAnalyzer import PatternAnalyzer\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 55\n",
    "BATCH = 5000\n",
    "NUN_THREADS = 6\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCH = 400\n",
    "LEARNING_RATE = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.5024, 0.1627, 0.1675, 0.9378, 0.2871, 0.0048, 0.6986, 0.0144,\n",
       "        0.0813, 0.1675, 0.1340, 0.1722, 0.3780, 0.4785, 0.7225, 0.2297, 0.1053,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MovieLensDataset(ml_path=\"ml-100k\", split=\"test\", seed=SEED)\n",
    "user_data, train_tensor, test_tensor = dataset.__getitem__(11)\n",
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_data shape: torch.Size([23])\n",
      "train_tensor shape: torch.Size([19, 22])\n",
      "test_tensor shape: torch.Size([19])\n"
     ]
    }
   ],
   "source": [
    "print(f\"user_data shape: {user_data.size()}\")\n",
    "print(f\"train_tensor shape: {train_tensor.size()}\")\n",
    "print(f\"test_tensor shape: {test_tensor.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  user_data_tensor shape: torch.Size([678, 23])\n",
      "  rating_train_tensor shape: torch.Size([678, 19, 22])\n",
      "  rating_test_tensor shape: torch.Size([678, 19])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MovieLensDataset(ml_path=\"ml-100k\", split=\"train\", seed=SEED)\n",
    "test_dataset = MovieLensDataset(ml_path=\"ml-100k\", split=\"test\", seed=SEED)\n",
    "val_dataset = MovieLensDataset(ml_path=\"ml-100k\", split=\"val\", seed=SEED)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=NUN_THREADS)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH, shuffle=True, num_workers=NUN_THREADS)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH, shuffle=True, num_workers=NUN_THREADS)\n",
    "\n",
    "try:\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        user_data_tensor, rating_train_tensor, rating_test_tensor = batch\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(\"  user_data_tensor shape:\", user_data_tensor.shape)\n",
    "        print(\"  rating_train_tensor shape:\", rating_train_tensor.shape)\n",
    "        print(\"  rating_test_tensor shape:\", rating_test_tensor.shape)\n",
    "        # Solo mostramos el primer batch\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(\"Error al iterar sobre el DataLoader:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1281,  0.6064, -0.4078,  ..., -1.6076, -1.1058,  1.4426],\n",
      "          [ 0.6030, -1.1032,  0.1374,  ..., -0.2090,  1.3200,  1.3305],\n",
      "          [ 0.7843,  1.0782, -0.7403,  ...,  1.0806,  0.4374, -0.1697],\n",
      "          ...,\n",
      "          [-1.4503, -1.1877,  0.8442,  ...,  0.8604,  1.2604,  1.3787],\n",
      "          [-0.7359, -0.2517,  0.2049,  ...,  1.6210, -0.3067,  0.6239],\n",
      "          [ 1.9142,  1.0068, -0.4572,  ..., -1.2460, -0.3769,  1.5491]]],\n",
      "\n",
      "\n",
      "        [[[-0.4804,  1.8881, -0.6675,  ...,  0.5989, -1.7020, -2.2787],\n",
      "          [-1.2628,  1.3608, -0.3817,  ..., -2.2630, -0.9694,  0.4777],\n",
      "          [-0.5104, -0.1009,  0.2481,  ...,  0.4398,  1.2223,  0.8874],\n",
      "          ...,\n",
      "          [ 0.5461, -1.0378,  1.2874,  ...,  0.2650, -0.4102,  2.1520],\n",
      "          [ 0.8942, -2.2010,  1.4985,  ..., -1.2943,  0.2081, -0.0424],\n",
      "          [ 1.0874,  0.8067,  1.1076,  ..., -1.0154,  0.2729,  1.0720]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1588,  0.7361, -0.9114,  ..., -1.7064,  0.2851, -0.4459],\n",
      "          [ 0.6465,  0.7698,  0.2259,  ...,  0.1348,  0.6994,  0.7128],\n",
      "          [-2.1456,  0.1710,  0.6985,  ..., -1.0546,  1.0139,  0.1675],\n",
      "          ...,\n",
      "          [-0.7456, -0.1145,  0.9888,  ...,  0.9174, -0.6567,  1.5926],\n",
      "          [ 0.1337, -0.7972,  0.1447,  ..., -1.1228, -0.2743,  0.9417],\n",
      "          [-0.4357,  0.1836, -0.6871,  ..., -1.3786,  1.0050, -0.3158]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.2953, -0.8159,  0.9190,  ...,  0.1685,  1.0973,  0.6014],\n",
      "          [ 0.4586,  0.0843, -1.0346,  ..., -0.3022,  0.7682,  1.4549],\n",
      "          [ 0.0094,  2.7662, -0.6671,  ..., -0.4890, -0.5040, -0.5881],\n",
      "          ...,\n",
      "          [ 0.0078,  0.2369,  1.5012,  ...,  1.4109,  0.7171, -1.0739],\n",
      "          [-0.3229,  0.5820, -0.2166,  ..., -0.5070, -0.0734,  0.6566],\n",
      "          [ 0.5035,  1.1099, -0.0924,  ..., -0.6455, -1.3872,  0.9359]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3959, -1.1296, -0.7415,  ...,  1.1484,  1.4846,  0.0181],\n",
      "          [-1.4728, -0.5312,  0.1519,  ...,  0.1401, -0.4775,  1.7689],\n",
      "          [ 0.2897,  1.3193,  0.9545,  ..., -1.9901, -1.9254,  1.6516],\n",
      "          ...,\n",
      "          [ 0.2668, -1.4739, -1.0682,  ...,  0.3502,  0.2157, -0.6488],\n",
      "          [-2.3058,  0.1819,  1.1508,  ...,  0.5267, -1.2085,  0.3115],\n",
      "          [ 0.2371,  0.9461, -0.1803,  ..., -0.3942,  0.2943,  1.0827]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5038,  0.5287, -1.9740,  ..., -0.3764,  0.0587, -0.6122],\n",
      "          [-0.9571, -0.1173, -0.8676,  ...,  0.9286, -1.6904, -0.5634],\n",
      "          [ 0.1898,  0.2763, -0.7428,  ...,  0.3791,  0.3558,  2.0666],\n",
      "          ...,\n",
      "          [ 1.1556,  0.0439,  1.1075,  ..., -0.5044,  2.7602,  2.1635],\n",
      "          [ 0.9865,  0.0271, -0.1088,  ...,  0.1273, -0.9588,  1.3171],\n",
      "          [ 2.0641,  0.4641, -0.3313,  ..., -0.3178,  1.1212,  0.8852]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 1.8770,  1.4385,  0.6684,  0.5365,  0.7085],\n",
      "          [ 0.6888, -0.8370, -1.0222, -1.0406, -0.0985],\n",
      "          [ 0.5987, -0.4258, -1.5783, -0.2801, -0.1010],\n",
      "          [ 0.4997, -1.6832, -0.3715, -0.8591,  0.6180]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [-0.2292, -0.4155,  0.1705, -0.2455, -0.4155],\n",
      "          [ 0.1775,  0.1805,  0.0217,  0.1916, -0.4155],\n",
      "          [ 0.6183, -0.2839,  1.0030,  0.6776,  0.0703]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6322, -1.0531, -0.8448, -1.0083, -2.1047],\n",
      "          [ 0.2241,  0.8343,  0.3210, -0.4403, -1.3156],\n",
      "          [-0.1635,  0.5206,  0.4037,  0.1128, -0.9298],\n",
      "          [ 2.1773,  0.5756,  1.0089,  0.5655,  1.0535]],\n",
      "\n",
      "         [[-1.2872, -0.1408,  0.1073,  1.5019,  1.3389],\n",
      "          [-1.8924, -0.7937,  0.2145, -0.7154,  1.2570],\n",
      "          [-1.6089, -0.6466, -0.1763, -0.1917,  0.7733],\n",
      "          [-1.0085,  0.4471,  0.5093,  0.1577,  0.1803]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8895,  1.0434,  1.1590,  1.0564,  1.0689],\n",
      "          [ 0.7743, -1.2031, -0.5596, -0.6888,  0.9394],\n",
      "          [ 0.9225, -0.1899, -1.3922,  0.0837,  0.1992],\n",
      "          [ 1.2992, -1.4296,  0.2760, -0.8608, -0.0147]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155,  0.1082],\n",
      "          [-0.4155, -0.4155, -0.1356, -0.4155,  0.2205],\n",
      "          [ 0.5731,  0.6764, -0.4155,  2.0611,  0.3606],\n",
      "          [-0.1976, -0.4155, -0.4155, -0.4155, -0.1999]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3292,  0.2148, -0.1829, -0.1727, -1.0362],\n",
      "          [-0.2070,  1.2719,  0.1602,  0.4757, -1.1028],\n",
      "          [ 0.0054, -0.2104,  1.3962,  0.6997, -0.8791],\n",
      "          [ 1.5120,  0.0708,  1.8753,  0.8114,  0.1574]],\n",
      "\n",
      "         [[-0.6934,  1.0759,  1.1246,  0.8049,  0.7692],\n",
      "          [-1.6953, -0.1920, -0.1686, -0.6146,  0.2720],\n",
      "          [-1.5895, -0.4705, -0.0159,  0.0203, -0.0274],\n",
      "          [-1.5626, -0.0206, -0.5433,  0.2214,  0.8933]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8061,  0.6038,  1.0173,  1.0043,  0.8283],\n",
      "          [ 0.2968, -0.8537, -1.4336, -0.6515, -0.0988],\n",
      "          [ 0.7426, -0.4181, -1.0450, -0.7261, -0.0614],\n",
      "          [ 0.8703, -0.8914, -1.1617, -0.4491, -0.1856]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 1.7670, -0.4155, -0.4155,  0.5400,  0.1122],\n",
      "          [ 0.5692, -0.3588, -0.3592, -0.2505, -0.4155],\n",
      "          [-0.2711, -0.3597, -0.4155, -0.4155, -0.4155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4424, -1.3759, -0.7548, -0.4577, -1.7799],\n",
      "          [-0.2003,  0.4530, -0.1500, -0.2039, -1.2698],\n",
      "          [ 0.4809,  0.1433, -0.4227,  0.0312, -1.2837],\n",
      "          [ 1.2643,  0.9055,  0.4568,  0.4700,  0.0463]],\n",
      "\n",
      "         [[-0.5740,  0.7078,  0.5819,  0.7949,  1.2161],\n",
      "          [-1.6236, -0.4046, -0.2177,  0.2341,  0.1636],\n",
      "          [-1.5927,  0.5267, -0.0580,  0.3546,  0.5695],\n",
      "          [-1.8047, -0.1860, -0.5647,  0.1777,  0.4380]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.5401,  0.9357,  0.5116,  0.5288,  1.1249],\n",
      "          [ 0.8130, -0.4673, -0.8923, -0.7676, -0.1815],\n",
      "          [ 0.4776, -0.1332, -0.8009, -0.4886,  0.2211],\n",
      "          [ 0.9809, -0.7181, -0.5859, -0.3144, -0.0745]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 0.2613, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 1.3776, -0.4155, -0.4155,  0.3767, -0.2560],\n",
      "          [-0.4155, -0.4155, -0.4155, -0.4155,  0.1051]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1412, -0.0657, -0.2970, -0.3000, -1.6859],\n",
      "          [-0.5195,  0.9053, -0.1694, -0.1963, -0.3937],\n",
      "          [ 0.1178,  0.1010, -0.0650,  0.5428, -1.4223],\n",
      "          [ 1.5795,  0.6997,  0.2654,  0.7597, -0.3319]],\n",
      "\n",
      "         [[-0.5871,  0.3271,  0.3649,  0.3769,  0.7858],\n",
      "          [-2.1259, -0.1200,  0.4479, -0.1284,  0.8786],\n",
      "          [-1.3093, -0.0118,  0.1833,  0.2442,  0.6270],\n",
      "          [-2.0874,  0.0672,  0.0977,  0.3253,  0.9888]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1817,  1.1381,  0.6624,  1.5704,  0.8944],\n",
      "          [ 0.9512, -0.9143, -0.9627, -1.3943, -0.2435],\n",
      "          [ 0.4625, -1.1944, -0.7298, -0.9054,  0.8812],\n",
      "          [ 1.3286, -0.3774, -0.8947, -0.6181,  0.0678]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 1.1489, -0.4155, -0.4155, -0.1231, -0.4155],\n",
      "          [ 0.8723, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 0.2321, -0.3329, -0.4155, -0.4155,  0.3041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2797, -1.0164, -0.6591,  0.5074, -1.3552],\n",
      "          [ 0.5153,  0.1757, -0.3748,  0.6059, -0.4005],\n",
      "          [-0.1820,  0.0471,  0.8252,  0.5982, -1.0429],\n",
      "          [ 0.7035,  1.1711, -0.0326,  1.6998, -0.1629]],\n",
      "\n",
      "         [[-0.4079,  0.2378,  0.2678,  0.4392,  1.1986],\n",
      "          [-2.0475,  0.0748,  0.2747,  0.0521,  0.6873],\n",
      "          [-1.5385, -0.4394,  0.2975,  0.4149,  0.3327],\n",
      "          [-2.0560, -0.3631, -0.3988, -0.0281,  0.4380]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9795,  1.9534,  1.1892,  1.2053,  0.8038],\n",
      "          [ 0.5295, -1.4804, -0.8431, -0.8931, -0.6552],\n",
      "          [ 0.7269, -0.1768, -0.4030, -1.5173, -0.0662],\n",
      "          [ 0.4176, -0.7458, -0.6505, -0.3070, -0.6240]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [-0.2801, -0.2171,  0.5685, -0.4155, -0.4155],\n",
      "          [ 0.4314, -0.2325, -0.4155, -0.2126,  0.1458],\n",
      "          [ 0.5921,  0.3620,  0.3798, -0.0549, -0.4155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4918, -0.8006, -1.0004, -0.6201, -0.7544],\n",
      "          [ 0.3194, -0.0419, -0.5236,  0.1346, -1.0155],\n",
      "          [-0.1013,  0.7469,  0.3156, -0.1462, -1.2217],\n",
      "          [ 0.7441,  0.8419,  0.6521,  0.9245, -0.3113]],\n",
      "\n",
      "         [[ 0.0239,  0.0829,  0.5933,  0.5269,  0.5936],\n",
      "          [-1.8500, -0.4132,  0.2136,  0.1779,  0.1949],\n",
      "          [-1.6619, -0.1607, -0.1881, -0.8637,  1.1392],\n",
      "          [-1.3281,  0.3037, -0.5026, -0.0610,  0.8854]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnBatchNormBackward0>)\n",
      "tensor([[[[ 1.8770,  1.4385,  0.6684,  0.5365,  0.7085],\n",
      "          [ 0.6888, -0.8370, -1.0222, -1.0406, -0.0985],\n",
      "          [ 0.5987, -0.4258, -1.5783, -0.2801, -0.1010],\n",
      "          [ 0.4997, -1.6832, -0.3715, -0.8591,  0.6180]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [-0.2292, -0.4155,  0.1705, -0.2455, -0.4155],\n",
      "          [ 0.1775,  0.1805,  0.0217,  0.1916, -0.4155],\n",
      "          [ 0.6183, -0.2839,  1.0030,  0.6776,  0.0703]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6322, -1.0531, -0.8448, -1.0083, -2.1047],\n",
      "          [ 0.2241,  0.8343,  0.3210, -0.4403, -1.3156],\n",
      "          [-0.1635,  0.5206,  0.4037,  0.1128, -0.9298],\n",
      "          [ 2.1773,  0.5756,  1.0089,  0.5655,  1.0535]],\n",
      "\n",
      "         [[-1.2872, -0.1408,  0.1073,  1.5019,  1.3389],\n",
      "          [-1.8924, -0.7937,  0.2145, -0.7154,  1.2570],\n",
      "          [-1.6089, -0.6466, -0.1763, -0.1917,  0.7733],\n",
      "          [-1.0085,  0.4471,  0.5093,  0.1577,  0.1803]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8895,  1.0434,  1.1590,  1.0564,  1.0689],\n",
      "          [ 0.7743, -1.2031, -0.5596, -0.6888,  0.9394],\n",
      "          [ 0.9225, -0.1899, -1.3922,  0.0837,  0.1992],\n",
      "          [ 1.2992, -1.4296,  0.2760, -0.8608, -0.0147]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155,  0.1082],\n",
      "          [-0.4155, -0.4155, -0.1356, -0.4155,  0.2205],\n",
      "          [ 0.5731,  0.6764, -0.4155,  2.0611,  0.3606],\n",
      "          [-0.1976, -0.4155, -0.4155, -0.4155, -0.1999]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3292,  0.2148, -0.1829, -0.1727, -1.0362],\n",
      "          [-0.2070,  1.2719,  0.1602,  0.4757, -1.1028],\n",
      "          [ 0.0054, -0.2104,  1.3962,  0.6997, -0.8791],\n",
      "          [ 1.5120,  0.0708,  1.8753,  0.8114,  0.1574]],\n",
      "\n",
      "         [[-0.6934,  1.0759,  1.1246,  0.8049,  0.7692],\n",
      "          [-1.6953, -0.1920, -0.1686, -0.6146,  0.2720],\n",
      "          [-1.5895, -0.4705, -0.0159,  0.0203, -0.0274],\n",
      "          [-1.5626, -0.0206, -0.5433,  0.2214,  0.8933]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8061,  0.6038,  1.0173,  1.0043,  0.8283],\n",
      "          [ 0.2968, -0.8537, -1.4336, -0.6515, -0.0988],\n",
      "          [ 0.7426, -0.4181, -1.0450, -0.7261, -0.0614],\n",
      "          [ 0.8703, -0.8914, -1.1617, -0.4491, -0.1856]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 1.7670, -0.4155, -0.4155,  0.5400,  0.1122],\n",
      "          [ 0.5692, -0.3588, -0.3592, -0.2505, -0.4155],\n",
      "          [-0.2711, -0.3597, -0.4155, -0.4155, -0.4155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4424, -1.3759, -0.7548, -0.4577, -1.7799],\n",
      "          [-0.2003,  0.4530, -0.1500, -0.2039, -1.2698],\n",
      "          [ 0.4809,  0.1433, -0.4227,  0.0312, -1.2837],\n",
      "          [ 1.2643,  0.9055,  0.4568,  0.4700,  0.0463]],\n",
      "\n",
      "         [[-0.5740,  0.7078,  0.5819,  0.7949,  1.2161],\n",
      "          [-1.6236, -0.4046, -0.2177,  0.2341,  0.1636],\n",
      "          [-1.5927,  0.5267, -0.0580,  0.3546,  0.5695],\n",
      "          [-1.8047, -0.1860, -0.5647,  0.1777,  0.4380]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.5401,  0.9357,  0.5116,  0.5288,  1.1249],\n",
      "          [ 0.8130, -0.4673, -0.8923, -0.7676, -0.1815],\n",
      "          [ 0.4776, -0.1332, -0.8009, -0.4886,  0.2211],\n",
      "          [ 0.9809, -0.7181, -0.5859, -0.3144, -0.0745]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 0.2613, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 1.3776, -0.4155, -0.4155,  0.3767, -0.2560],\n",
      "          [-0.4155, -0.4155, -0.4155, -0.4155,  0.1051]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1412, -0.0657, -0.2970, -0.3000, -1.6859],\n",
      "          [-0.5195,  0.9053, -0.1694, -0.1963, -0.3937],\n",
      "          [ 0.1178,  0.1010, -0.0650,  0.5428, -1.4223],\n",
      "          [ 1.5795,  0.6997,  0.2654,  0.7597, -0.3319]],\n",
      "\n",
      "         [[-0.5871,  0.3271,  0.3649,  0.3769,  0.7858],\n",
      "          [-2.1259, -0.1200,  0.4479, -0.1284,  0.8786],\n",
      "          [-1.3093, -0.0118,  0.1833,  0.2442,  0.6270],\n",
      "          [-2.0874,  0.0672,  0.0977,  0.3253,  0.9888]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1817,  1.1381,  0.6624,  1.5704,  0.8944],\n",
      "          [ 0.9512, -0.9143, -0.9627, -1.3943, -0.2435],\n",
      "          [ 0.4625, -1.1944, -0.7298, -0.9054,  0.8812],\n",
      "          [ 1.3286, -0.3774, -0.8947, -0.6181,  0.0678]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 1.1489, -0.4155, -0.4155, -0.1231, -0.4155],\n",
      "          [ 0.8723, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [ 0.2321, -0.3329, -0.4155, -0.4155,  0.3041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2797, -1.0164, -0.6591,  0.5074, -1.3552],\n",
      "          [ 0.5153,  0.1757, -0.3748,  0.6059, -0.4005],\n",
      "          [-0.1820,  0.0471,  0.8252,  0.5982, -1.0429],\n",
      "          [ 0.7035,  1.1711, -0.0326,  1.6998, -0.1629]],\n",
      "\n",
      "         [[-0.4079,  0.2378,  0.2678,  0.4392,  1.1986],\n",
      "          [-2.0475,  0.0748,  0.2747,  0.0521,  0.6873],\n",
      "          [-1.5385, -0.4394,  0.2975,  0.4149,  0.3327],\n",
      "          [-2.0560, -0.3631, -0.3988, -0.0281,  0.4380]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9795,  1.9534,  1.1892,  1.2053,  0.8038],\n",
      "          [ 0.5295, -1.4804, -0.8431, -0.8931, -0.6552],\n",
      "          [ 0.7269, -0.1768, -0.4030, -1.5173, -0.0662],\n",
      "          [ 0.4176, -0.7458, -0.6505, -0.3070, -0.6240]],\n",
      "\n",
      "         [[-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027],\n",
      "          [-0.0027, -0.0027, -0.0027, -0.0027, -0.0027]],\n",
      "\n",
      "         [[-0.4155, -0.4155, -0.4155, -0.4155, -0.4155],\n",
      "          [-0.2801, -0.2171,  0.5685, -0.4155, -0.4155],\n",
      "          [ 0.4314, -0.2325, -0.4155, -0.2126,  0.1458],\n",
      "          [ 0.5921,  0.3620,  0.3798, -0.0549, -0.4155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4918, -0.8006, -1.0004, -0.6201, -0.7544],\n",
      "          [ 0.3194, -0.0419, -0.5236,  0.1346, -1.0155],\n",
      "          [-0.1013,  0.7469,  0.3156, -0.1462, -1.2217],\n",
      "          [ 0.7441,  0.8419,  0.6521,  0.9245, -0.3113]],\n",
      "\n",
      "         [[ 0.0239,  0.0829,  0.5933,  0.5269,  0.5936],\n",
      "          [-1.8500, -0.4132,  0.2136,  0.1779,  0.1949],\n",
      "          [-1.6619, -0.1607, -0.1881, -0.8637,  1.1392],\n",
      "          [-1.3281,  0.3037, -0.5026, -0.0610,  0.8854]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnBatchNormBackward0>)\n",
      "tensor([[0.0444, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0450, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0448, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0449, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0445, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0445, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "Forma de la salida después de `view`: torch.Size([628, 19])\n",
      "Model parameters: 298669\n"
     ]
    }
   ],
   "source": [
    "height=19\n",
    "width=22\n",
    "user_input_size = 23\n",
    "    # Crear instancia del modelo\n",
    "pattern_analyzer = PatternAnalyzer(conv_structure=[1,4,8,16,32,32,32],\n",
    "                            input_size=torch.Size((height, width)),\n",
    "                            pool_depth=3,\n",
    "                            expert_hidden_size=16,\n",
    "                            expert_output_len=4,\n",
    "                            final_mlp_factor=2,\n",
    "                            final_mlp_output_len=15,\n",
    "                            ).to(DEVICE)\n",
    "    \n",
    "model = Recommender(\n",
    "        user_data_input_size=user_input_size,\n",
    "        user_data_analizer_factor = 2,\n",
    "        user_data_analizer_output_size = 10,\n",
    "        pattern_analyzer=pattern_analyzer,\n",
    "        final_regressor_factor=1,\n",
    "        final_regressor_output_len=19\n",
    "    ).to(DEVICE)\n",
    "\n",
    "ratings = torch.randn(628, height, width).to(DEVICE)\n",
    "user_data = torch.randn(628, user_input_size).to(DEVICE)\n",
    "    \n",
    "\n",
    "    # Pasar el tensor por el modelo\n",
    "output = model(user_data, ratings)\n",
    "print(output)\n",
    "    # Mostrar la forma de la salida final\n",
    "print(\"Forma de la salida después de `view`:\", output.shape)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el optimizador, por ejemplo, usando Adam con una tasa de aprendizaje de 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Crear la función de pérdida para regresión (Mean Squared Error Loss)\n",
    "criterion = nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[5.0000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.2676e-02],\n",
      "          [5.0000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 3.2938e-03],\n",
      "          [7.5000e-01, 5.9142e-04, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.4445e-02],\n",
      "          ...,\n",
      "          [5.0000e-01, 3.5532e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.5622e-01],\n",
      "          [5.0000e-01, 4.0144e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           1.0000e+00, 1.0243e-01],\n",
      "          [7.5000e-01, 4.0144e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 6.6517e-02]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "           0.0000e+00, 3.1564e-02],\n",
      "          [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 7.1734e-03],\n",
      "          [1.0000e+00, 2.0919e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 2.6747e-02],\n",
      "          ...,\n",
      "          [1.0000e+00, 4.6203e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 9.6259e-01],\n",
      "          [1.0000e+00, 4.7885e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 2.8695e-02],\n",
      "          [1.0000e+00, 4.9341e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.3907e-01]]],\n",
      "\n",
      "\n",
      "        [[[7.5000e-01, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "           0.0000e+00, 9.3688e-03],\n",
      "          [7.5000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 7.4625e-03],\n",
      "          [1.0000e+00, 8.1505e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 7.4626e-03],\n",
      "          ...,\n",
      "          [1.0000e+00, 7.4138e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 6.7412e-01],\n",
      "          [5.0000e-01, 7.4138e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 3.7111e-02],\n",
      "          [7.5000e-01, 8.3386e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.5188e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.0151e-02],\n",
      "          [2.5000e-01, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
      "           0.0000e+00, 1.1849e-02],\n",
      "          [1.0000e+00, 8.5508e-06, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.5613e-03],\n",
      "          ...,\n",
      "          [5.0000e-01, 5.2814e-05, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.9184e-02],\n",
      "          [1.0000e+00, 5.5706e-05, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.3367e-02],\n",
      "          [5.0000e-01, 5.5832e-05, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 3.5033e-03]]],\n",
      "\n",
      "\n",
      "        [[[5.0000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 5.2393e-03],\n",
      "          [7.5000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [7.5000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 9.4962e-03],\n",
      "          ...,\n",
      "          [1.0000e+00, 6.0735e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 6.8768e-03],\n",
      "          [7.5000e-01, 6.0735e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.5141e-03],\n",
      "          [7.5000e-01, 6.0851e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 2.2922e-02]]],\n",
      "\n",
      "\n",
      "        [[[7.5000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 5.7202e-02],\n",
      "          [5.0000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 6.6161e-02],\n",
      "          [5.0000e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 6.4980e-02],\n",
      "          ...,\n",
      "          [1.0000e+00, 4.6147e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 9.6406e-01],\n",
      "          [7.5000e-01, 4.6147e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.3695e-01],\n",
      "          [1.0000e+00, 4.6147e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 9.6486e-02]]]], device='cuda:0')\n",
      "tensor([[[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnBatchNormBackward0>)\n",
      "tensor([[[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnBatchNormBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m running_val_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_data_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating_test_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_data_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating_train_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m        \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating_test_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1420\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1421\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1422\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    for user_data_tensor, rating_train_tensor, rating_test_tensor in train_dataloader:\n",
    "        optimizer.zero_grad()   \n",
    "             \n",
    "        outputs = model(user_data_tensor.to(DEVICE), rating_train_tensor.to(DEVICE)).to(DEVICE)        \n",
    "        \n",
    "        loss = criterion(outputs.to(DEVICE), rating_test_tensor.to(DEVICE)) \n",
    "        \n",
    "        loss.backward()                 # Backpropagation\n",
    "        optimizer.step()                # Update model parameters\n",
    "        running_train_loss += loss.item() * rating_test_tensor.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_train_loss / len(train_dataloader.dataset)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for user_data_tensor, rating_train_tensor, rating_test_tensor in val_dataloader:\n",
    "            \n",
    "            outputs = model(user_data_tensor.to(DEVICE), rating_train_tensor.to(DEVICE))        \n",
    "            loss = criterion(outputs, rating_test_tensor.to(DEVICE)) \n",
    "            \n",
    "            running_val_loss += loss.item() * rating_test_tensor.size(0)\n",
    "    \n",
    "    epoch_val_loss = running_val_loss / len(val_dataloader.dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCH}] - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
