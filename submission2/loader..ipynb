{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Literal\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from utils.loader import MovieLensDataset\n",
    "from arquitecture.Recommender import Recommender\n",
    "from arquitecture.components.PatterAnalyzer import PatternAnalyzer\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 55\n",
    "BATCH = 2000\n",
    "NUN_THREADS = 6\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCH = 400\n",
    "LEARNING_RATE = 0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieLensDataset(ml_path=\"ml-100k\", split=\"train\", seed=SEED)\n",
    "test_dataset = MovieLensDataset(ml_path=\"ml-100k\", split=\"test\", seed=SEED)\n",
    "val_dataset = MovieLensDataset(ml_path=\"ml-100k\", split=\"val\", seed=SEED)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=NUN_THREADS)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH, shuffle=True, num_workers=NUN_THREADS)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH, shuffle=True, num_workers=NUN_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 513534\n"
     ]
    }
   ],
   "source": [
    "height=19\n",
    "width=22\n",
    "user_input_size = 23\n",
    "    # Crear instancia del modelo\n",
    "pattern_analyzer = PatternAnalyzer(conv_structure=[1,8,8,16,16,32,32],\n",
    "                            input_size=torch.Size((height, width)),\n",
    "                            pool_depth=3,\n",
    "                            expert_hidden_size=12,\n",
    "                            expert_output_len=8,\n",
    "                            final_mlp_factor=2,\n",
    "                            final_mlp_output_len=12,\n",
    "                            ).to(DEVICE)\n",
    "    \n",
    "model = Recommender(\n",
    "        user_data_input_size=user_input_size,\n",
    "        user_data_analizer_factor = 1,\n",
    "        user_data_analizer_output_size = 10,\n",
    "        pattern_analyzer=pattern_analyzer,\n",
    "        final_regressor_factor=1,\n",
    "        final_regressor_output_len=19\n",
    "    ).to(DEVICE)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el optimizador, por ejemplo, usando Adam con una tasa de aprendizaje de 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Crear la función de pérdida para regresión (Mean Squared Error Loss)\n",
    "criterion = nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400] - Train Loss: 38.3259, Val Loss: 38.8306\n",
      "Epoch [2/400] - Train Loss: 38.1760, Val Loss: 38.6755\n",
      "Epoch [3/400] - Train Loss: 38.0289, Val Loss: 38.5218\n",
      "Epoch [4/400] - Train Loss: 37.8833, Val Loss: 38.3681\n",
      "Epoch [5/400] - Train Loss: 37.7371, Val Loss: 38.2165\n",
      "Epoch [6/400] - Train Loss: 37.5917, Val Loss: 38.0724\n",
      "Epoch [7/400] - Train Loss: 37.4530, Val Loss: 37.9336\n",
      "Epoch [8/400] - Train Loss: 37.3198, Val Loss: 37.7947\n",
      "Epoch [9/400] - Train Loss: 37.1868, Val Loss: 37.6558\n",
      "Epoch [10/400] - Train Loss: 37.0540, Val Loss: 37.5132\n",
      "Epoch [11/400] - Train Loss: 36.9176, Val Loss: 37.3661\n",
      "Epoch [12/400] - Train Loss: 36.7770, Val Loss: 37.2177\n",
      "Epoch [13/400] - Train Loss: 36.6353, Val Loss: 37.0679\n",
      "Epoch [14/400] - Train Loss: 36.4924, Val Loss: 36.9132\n",
      "Epoch [15/400] - Train Loss: 36.3448, Val Loss: 36.7525\n",
      "Epoch [16/400] - Train Loss: 36.1917, Val Loss: 36.5822\n",
      "Epoch [17/400] - Train Loss: 36.0295, Val Loss: 36.3967\n",
      "Epoch [18/400] - Train Loss: 35.8525, Val Loss: 36.1935\n",
      "Epoch [19/400] - Train Loss: 35.6574, Val Loss: 35.9725\n",
      "Epoch [20/400] - Train Loss: 35.4458, Val Loss: 35.7295\n",
      "Epoch [21/400] - Train Loss: 35.2142, Val Loss: 35.4494\n",
      "Epoch [22/400] - Train Loss: 34.9459, Val Loss: 35.1400\n",
      "Epoch [23/400] - Train Loss: 34.6492, Val Loss: 34.7925\n",
      "Epoch [24/400] - Train Loss: 34.3163, Val Loss: 34.4049\n",
      "Epoch [25/400] - Train Loss: 33.9443, Val Loss: 33.9617\n",
      "Epoch [26/400] - Train Loss: 33.5207, Val Loss: 33.4539\n",
      "Epoch [27/400] - Train Loss: 33.0390, Val Loss: 33.0086\n",
      "Epoch [28/400] - Train Loss: 32.6051, Val Loss: 32.7356\n",
      "Epoch [29/400] - Train Loss: 32.3261, Val Loss: 32.4043\n",
      "Epoch [30/400] - Train Loss: 31.9922, Val Loss: 32.0105\n",
      "Epoch [31/400] - Train Loss: 31.6321, Val Loss: 31.6330\n",
      "Epoch [32/400] - Train Loss: 31.2962, Val Loss: 31.1632\n",
      "Epoch [33/400] - Train Loss: 30.8395, Val Loss: 30.6344\n",
      "Epoch [34/400] - Train Loss: 30.3066, Val Loss: 30.2279\n",
      "Epoch [35/400] - Train Loss: 29.8896, Val Loss: 29.9344\n",
      "Epoch [36/400] - Train Loss: 29.5897, Val Loss: 29.5912\n",
      "Epoch [37/400] - Train Loss: 29.2501, Val Loss: 29.1232\n",
      "Epoch [38/400] - Train Loss: 28.7963, Val Loss: 28.6416\n",
      "Epoch [39/400] - Train Loss: 28.3360, Val Loss: 28.1855\n",
      "Epoch [40/400] - Train Loss: 27.9108, Val Loss: 27.7845\n",
      "Epoch [41/400] - Train Loss: 27.5421, Val Loss: 27.3546\n",
      "Epoch [42/400] - Train Loss: 27.1304, Val Loss: 26.8780\n",
      "Epoch [43/400] - Train Loss: 26.6562, Val Loss: 26.4396\n",
      "Epoch [44/400] - Train Loss: 26.2140, Val Loss: 26.0382\n",
      "Epoch [45/400] - Train Loss: 25.8137, Val Loss: 25.6069\n",
      "Epoch [46/400] - Train Loss: 25.3952, Val Loss: 25.0581\n",
      "Epoch [47/400] - Train Loss: 24.8708, Val Loss: 24.1683\n",
      "Epoch [48/400] - Train Loss: 24.0219, Val Loss: 23.6018\n",
      "Epoch [49/400] - Train Loss: 23.5129, Val Loss: 23.2453\n",
      "Epoch [50/400] - Train Loss: 23.1912, Val Loss: 22.4836\n",
      "Epoch [51/400] - Train Loss: 22.4203, Val Loss: 21.8932\n",
      "Epoch [52/400] - Train Loss: 21.8050, Val Loss: 21.5336\n",
      "Epoch [53/400] - Train Loss: 21.4317, Val Loss: 21.0863\n",
      "Epoch [54/400] - Train Loss: 20.9894, Val Loss: 20.4840\n",
      "Epoch [55/400] - Train Loss: 20.4105, Val Loss: 19.9285\n",
      "Epoch [56/400] - Train Loss: 19.8914, Val Loss: 19.5838\n",
      "Epoch [57/400] - Train Loss: 19.5824, Val Loss: 19.2284\n",
      "Epoch [58/400] - Train Loss: 19.2400, Val Loss: 18.7757\n",
      "Epoch [59/400] - Train Loss: 18.7749, Val Loss: 18.4520\n",
      "Epoch [60/400] - Train Loss: 18.4304, Val Loss: 18.2294\n",
      "Epoch [61/400] - Train Loss: 18.1952, Val Loss: 17.9494\n",
      "Epoch [62/400] - Train Loss: 17.9176, Val Loss: 17.6142\n",
      "Epoch [63/400] - Train Loss: 17.5982, Val Loss: 17.3495\n",
      "Epoch [64/400] - Train Loss: 17.3551, Val Loss: 17.1767\n",
      "Epoch [65/400] - Train Loss: 17.1962, Val Loss: 16.9736\n",
      "Epoch [66/400] - Train Loss: 16.9882, Val Loss: 16.7672\n",
      "Epoch [67/400] - Train Loss: 16.7628, Val Loss: 16.6387\n",
      "Epoch [68/400] - Train Loss: 16.6142, Val Loss: 16.5279\n",
      "Epoch [69/400] - Train Loss: 16.4922, Val Loss: 16.3714\n",
      "Epoch [70/400] - Train Loss: 16.3363, Val Loss: 16.2104\n",
      "Epoch [71/400] - Train Loss: 16.1841, Val Loss: 16.1015\n",
      "Epoch [72/400] - Train Loss: 16.0840, Val Loss: 16.0092\n",
      "Epoch [73/400] - Train Loss: 15.9917, Val Loss: 15.9088\n",
      "Epoch [74/400] - Train Loss: 15.8800, Val Loss: 15.8334\n",
      "Epoch [75/400] - Train Loss: 15.7889, Val Loss: 15.7835\n",
      "Epoch [76/400] - Train Loss: 15.7267, Val Loss: 15.7174\n",
      "Epoch [77/400] - Train Loss: 15.6555, Val Loss: 15.6376\n",
      "Epoch [78/400] - Train Loss: 15.5775, Val Loss: 15.5783\n",
      "Epoch [79/400] - Train Loss: 15.5224, Val Loss: 15.5338\n",
      "Epoch [80/400] - Train Loss: 15.4774, Val Loss: 15.4897\n",
      "Epoch [81/400] - Train Loss: 15.4257, Val Loss: 15.4548\n",
      "Epoch [82/400] - Train Loss: 15.3801, Val Loss: 15.4288\n",
      "Epoch [83/400] - Train Loss: 15.3459, Val Loss: 15.3939\n",
      "Epoch [84/400] - Train Loss: 15.3083, Val Loss: 15.3537\n",
      "Epoch [85/400] - Train Loss: 15.2700, Val Loss: 15.3229\n",
      "Epoch [86/400] - Train Loss: 15.2416, Val Loss: 15.2970\n",
      "Epoch [87/400] - Train Loss: 15.2148, Val Loss: 15.2717\n",
      "Epoch [88/400] - Train Loss: 15.1845, Val Loss: 15.2539\n",
      "Epoch [89/400] - Train Loss: 15.1607, Val Loss: 15.2373\n",
      "Epoch [90/400] - Train Loss: 15.1411, Val Loss: 15.2141\n",
      "Epoch [91/400] - Train Loss: 15.1189, Val Loss: 15.1918\n",
      "Epoch [92/400] - Train Loss: 15.0999, Val Loss: 15.1757\n",
      "Epoch [93/400] - Train Loss: 15.0857, Val Loss: 15.1610\n",
      "Epoch [94/400] - Train Loss: 15.0694, Val Loss: 15.1500\n",
      "Epoch [95/400] - Train Loss: 15.0543, Val Loss: 15.1418\n",
      "Epoch [96/400] - Train Loss: 15.0431, Val Loss: 15.1293\n",
      "Epoch [97/400] - Train Loss: 15.0307, Val Loss: 15.1150\n",
      "Epoch [98/400] - Train Loss: 15.0188, Val Loss: 15.1044\n",
      "Epoch [99/400] - Train Loss: 15.0100, Val Loss: 15.0954\n",
      "Epoch [100/400] - Train Loss: 14.9999, Val Loss: 15.0887\n",
      "Epoch [101/400] - Train Loss: 14.9901, Val Loss: 15.0837\n",
      "Epoch [102/400] - Train Loss: 14.9824, Val Loss: 15.0749\n",
      "Epoch [103/400] - Train Loss: 14.9736, Val Loss: 15.0651\n",
      "Epoch [104/400] - Train Loss: 14.9657, Val Loss: 15.0573\n",
      "Epoch [105/400] - Train Loss: 14.9590, Val Loss: 15.0506\n",
      "Epoch [106/400] - Train Loss: 14.9506, Val Loss: 15.0456\n",
      "Epoch [107/400] - Train Loss: 14.9438, Val Loss: 15.0376\n",
      "Epoch [108/400] - Train Loss: 14.9364, Val Loss: 15.0274\n",
      "Epoch [109/400] - Train Loss: 14.9290, Val Loss: 15.0187\n",
      "Epoch [110/400] - Train Loss: 14.9222, Val Loss: 15.0115\n",
      "Epoch [111/400] - Train Loss: 14.9142, Val Loss: 15.0047\n",
      "Epoch [112/400] - Train Loss: 14.9072, Val Loss: 14.9939\n",
      "Epoch [113/400] - Train Loss: 14.8991, Val Loss: 14.9830\n",
      "Epoch [114/400] - Train Loss: 14.8917, Val Loss: 14.9738\n",
      "Epoch [115/400] - Train Loss: 14.8828, Val Loss: 14.9656\n",
      "Epoch [116/400] - Train Loss: 14.8742, Val Loss: 14.9535\n",
      "Epoch [117/400] - Train Loss: 14.8647, Val Loss: 14.9411\n",
      "Epoch [118/400] - Train Loss: 14.8555, Val Loss: 14.9311\n",
      "Epoch [119/400] - Train Loss: 14.8452, Val Loss: 14.9203\n",
      "Epoch [120/400] - Train Loss: 14.8351, Val Loss: 14.9063\n",
      "Epoch [121/400] - Train Loss: 14.8245, Val Loss: 14.8951\n",
      "Epoch [122/400] - Train Loss: 14.8131, Val Loss: 14.8840\n",
      "Epoch [123/400] - Train Loss: 14.8014, Val Loss: 14.8691\n",
      "Epoch [124/400] - Train Loss: 14.7892, Val Loss: 14.8585\n",
      "Epoch [125/400] - Train Loss: 14.7761, Val Loss: 14.8444\n",
      "Epoch [126/400] - Train Loss: 14.7624, Val Loss: 14.8308\n",
      "Epoch [127/400] - Train Loss: 14.7484, Val Loss: 14.8208\n",
      "Epoch [128/400] - Train Loss: 14.7345, Val Loss: 14.8036\n",
      "Epoch [129/400] - Train Loss: 14.7217, Val Loss: 14.8083\n",
      "Epoch [130/400] - Train Loss: 14.7137, Val Loss: 14.7836\n",
      "Epoch [131/400] - Train Loss: 14.7024, Val Loss: 14.7843\n",
      "Epoch [132/400] - Train Loss: 14.6868, Val Loss: 14.7632\n",
      "Epoch [133/400] - Train Loss: 14.6699, Val Loss: 14.7535\n",
      "Epoch [134/400] - Train Loss: 14.6648, Val Loss: 14.7717\n",
      "Epoch [135/400] - Train Loss: 14.6632, Val Loss: 14.7377\n",
      "Epoch [136/400] - Train Loss: 14.6441, Val Loss: 14.7321\n",
      "Epoch [137/400] - Train Loss: 14.6311, Val Loss: 14.7437\n",
      "Epoch [138/400] - Train Loss: 14.6294, Val Loss: 14.7221\n",
      "Epoch [139/400] - Train Loss: 14.6228, Val Loss: 14.7298\n",
      "Epoch [140/400] - Train Loss: 14.6105, Val Loss: 14.7142\n",
      "Epoch [141/400] - Train Loss: 14.5984, Val Loss: 14.7077\n",
      "Epoch [142/400] - Train Loss: 14.5950, Val Loss: 14.7302\n",
      "Epoch [143/400] - Train Loss: 14.5949, Val Loss: 14.7027\n",
      "Epoch [144/400] - Train Loss: 14.5853, Val Loss: 14.7118\n",
      "Epoch [145/400] - Train Loss: 14.5734, Val Loss: 14.7040\n",
      "Epoch [146/400] - Train Loss: 14.5649, Val Loss: 14.6975\n",
      "Epoch [147/400] - Train Loss: 14.5625, Val Loss: 14.7231\n",
      "Epoch [148/400] - Train Loss: 14.5618, Val Loss: 14.6967\n",
      "Epoch [149/400] - Train Loss: 14.5536, Val Loss: 14.7116\n",
      "Epoch [150/400] - Train Loss: 14.5434, Val Loss: 14.7032\n",
      "Epoch [151/400] - Train Loss: 14.5351, Val Loss: 14.6994\n",
      "Epoch [152/400] - Train Loss: 14.5316, Val Loss: 14.7249\n",
      "Epoch [153/400] - Train Loss: 14.5300, Val Loss: 14.7020\n",
      "Epoch [154/400] - Train Loss: 14.5240, Val Loss: 14.7244\n",
      "Epoch [155/400] - Train Loss: 14.5158, Val Loss: 14.7117\n",
      "Epoch [156/400] - Train Loss: 14.5074, Val Loss: 14.7132\n",
      "Epoch [157/400] - Train Loss: 14.5030, Val Loss: 14.7340\n",
      "Epoch [158/400] - Train Loss: 14.5011, Val Loss: 14.7151\n",
      "Epoch [159/400] - Train Loss: 14.4975, Val Loss: 14.7395\n",
      "Epoch [160/400] - Train Loss: 14.4920, Val Loss: 14.7223\n",
      "Epoch [161/400] - Train Loss: 14.4848, Val Loss: 14.7278\n",
      "Epoch [162/400] - Train Loss: 14.4794, Val Loss: 14.7374\n",
      "Epoch [163/400] - Train Loss: 14.4761, Val Loss: 14.7228\n",
      "Epoch [164/400] - Train Loss: 14.4732, Val Loss: 14.7462\n",
      "Epoch [165/400] - Train Loss: 14.4696, Val Loss: 14.7235\n",
      "Epoch [166/400] - Train Loss: 14.4641, Val Loss: 14.7387\n",
      "Epoch [167/400] - Train Loss: 14.4585, Val Loss: 14.7299\n",
      "Epoch [168/400] - Train Loss: 14.4534, Val Loss: 14.7292\n",
      "Epoch [169/400] - Train Loss: 14.4494, Val Loss: 14.7400\n",
      "Epoch [170/400] - Train Loss: 14.4461, Val Loss: 14.7244\n",
      "Epoch [171/400] - Train Loss: 14.4429, Val Loss: 14.7487\n",
      "Epoch [172/400] - Train Loss: 14.4400, Val Loss: 14.7224\n",
      "Epoch [173/400] - Train Loss: 14.4362, Val Loss: 14.7513\n",
      "Epoch [174/400] - Train Loss: 14.4325, Val Loss: 14.7228\n",
      "Epoch [175/400] - Train Loss: 14.4276, Val Loss: 14.7485\n",
      "Epoch [176/400] - Train Loss: 14.4225, Val Loss: 14.7260\n",
      "Epoch [177/400] - Train Loss: 14.4167, Val Loss: 14.7403\n",
      "Epoch [178/400] - Train Loss: 14.4113, Val Loss: 14.7315\n",
      "Epoch [179/400] - Train Loss: 14.4061, Val Loss: 14.7320\n",
      "Epoch [180/400] - Train Loss: 14.4014, Val Loss: 14.7358\n",
      "Epoch [181/400] - Train Loss: 14.3969, Val Loss: 14.7250\n",
      "Epoch [182/400] - Train Loss: 14.3926, Val Loss: 14.7391\n",
      "Epoch [183/400] - Train Loss: 14.3885, Val Loss: 14.7175\n",
      "Epoch [184/400] - Train Loss: 14.3847, Val Loss: 14.7466\n",
      "Epoch [185/400] - Train Loss: 14.3822, Val Loss: 14.7109\n",
      "Epoch [186/400] - Train Loss: 14.3800, Val Loss: 14.7615\n",
      "Epoch [187/400] - Train Loss: 14.3802, Val Loss: 14.7086\n",
      "Epoch [188/400] - Train Loss: 14.3754, Val Loss: 14.7604\n",
      "Epoch [189/400] - Train Loss: 14.3694, Val Loss: 14.7112\n",
      "Epoch [190/400] - Train Loss: 14.3564, Val Loss: 14.7282\n",
      "Epoch [191/400] - Train Loss: 14.3463, Val Loss: 14.7326\n",
      "Epoch [192/400] - Train Loss: 14.3417, Val Loss: 14.7133\n",
      "Epoch [193/400] - Train Loss: 14.3403, Val Loss: 14.7538\n",
      "Epoch [194/400] - Train Loss: 14.3380, Val Loss: 14.7134\n",
      "Epoch [195/400] - Train Loss: 14.3298, Val Loss: 14.7371\n",
      "Epoch [196/400] - Train Loss: 14.3202, Val Loss: 14.7247\n",
      "Epoch [197/400] - Train Loss: 14.3124, Val Loss: 14.7157\n",
      "Epoch [198/400] - Train Loss: 14.3079, Val Loss: 14.7405\n",
      "Epoch [199/400] - Train Loss: 14.3045, Val Loss: 14.7085\n",
      "Epoch [200/400] - Train Loss: 14.2985, Val Loss: 14.7322\n",
      "Epoch [201/400] - Train Loss: 14.2906, Val Loss: 14.7105\n",
      "Epoch [202/400] - Train Loss: 14.2818, Val Loss: 14.7108\n",
      "Epoch [203/400] - Train Loss: 14.2749, Val Loss: 14.7210\n",
      "Epoch [204/400] - Train Loss: 14.2698, Val Loss: 14.6999\n",
      "Epoch [205/400] - Train Loss: 14.2644, Val Loss: 14.7206\n",
      "Epoch [206/400] - Train Loss: 14.2580, Val Loss: 14.6964\n",
      "Epoch [207/400] - Train Loss: 14.2500, Val Loss: 14.7048\n",
      "Epoch [208/400] - Train Loss: 14.2422, Val Loss: 14.6974\n",
      "Epoch [209/400] - Train Loss: 14.2351, Val Loss: 14.6874\n",
      "Epoch [210/400] - Train Loss: 14.2289, Val Loss: 14.6980\n",
      "Epoch [211/400] - Train Loss: 14.2230, Val Loss: 14.6771\n",
      "Epoch [212/400] - Train Loss: 14.2165, Val Loss: 14.6907\n",
      "Epoch [213/400] - Train Loss: 14.2095, Val Loss: 14.6742\n",
      "Epoch [214/400] - Train Loss: 14.2020, Val Loss: 14.6789\n",
      "Epoch [215/400] - Train Loss: 14.1949, Val Loss: 14.6762\n",
      "Epoch [216/400] - Train Loss: 14.1883, Val Loss: 14.6686\n",
      "Epoch [217/400] - Train Loss: 14.1822, Val Loss: 14.6763\n",
      "Epoch [218/400] - Train Loss: 14.1762, Val Loss: 14.6599\n",
      "Epoch [219/400] - Train Loss: 14.1699, Val Loss: 14.6691\n",
      "Epoch [220/400] - Train Loss: 14.1633, Val Loss: 14.6535\n",
      "Epoch [221/400] - Train Loss: 14.1566, Val Loss: 14.6584\n",
      "Epoch [222/400] - Train Loss: 14.1501, Val Loss: 14.6506\n",
      "Epoch [223/400] - Train Loss: 14.1436, Val Loss: 14.6491\n",
      "Epoch [224/400] - Train Loss: 14.1373, Val Loss: 14.6509\n",
      "Epoch [225/400] - Train Loss: 14.1312, Val Loss: 14.6436\n",
      "Epoch [226/400] - Train Loss: 14.1250, Val Loss: 14.6493\n",
      "Epoch [227/400] - Train Loss: 14.1187, Val Loss: 14.6375\n",
      "Epoch [228/400] - Train Loss: 14.1124, Val Loss: 14.6434\n",
      "Epoch [229/400] - Train Loss: 14.1061, Val Loss: 14.6296\n",
      "Epoch [230/400] - Train Loss: 14.0997, Val Loss: 14.6341\n",
      "Epoch [231/400] - Train Loss: 14.0933, Val Loss: 14.6221\n",
      "Epoch [232/400] - Train Loss: 14.0868, Val Loss: 14.6282\n",
      "Epoch [233/400] - Train Loss: 14.0803, Val Loss: 14.6176\n",
      "Epoch [234/400] - Train Loss: 14.0737, Val Loss: 14.6255\n",
      "Epoch [235/400] - Train Loss: 14.0671, Val Loss: 14.6116\n",
      "Epoch [236/400] - Train Loss: 14.0607, Val Loss: 14.6240\n",
      "Epoch [237/400] - Train Loss: 14.0546, Val Loss: 14.6023\n",
      "Epoch [238/400] - Train Loss: 14.0487, Val Loss: 14.6258\n",
      "Epoch [239/400] - Train Loss: 14.0440, Val Loss: 14.5934\n",
      "Epoch [240/400] - Train Loss: 14.0402, Val Loss: 14.6390\n",
      "Epoch [241/400] - Train Loss: 14.0389, Val Loss: 14.5920\n",
      "Epoch [242/400] - Train Loss: 14.0346, Val Loss: 14.6455\n",
      "Epoch [243/400] - Train Loss: 14.0290, Val Loss: 14.5933\n",
      "Epoch [244/400] - Train Loss: 14.0134, Val Loss: 14.6106\n",
      "Epoch [245/400] - Train Loss: 13.9999, Val Loss: 14.6073\n",
      "Epoch [246/400] - Train Loss: 13.9929, Val Loss: 14.5893\n",
      "Epoch [247/400] - Train Loss: 13.9909, Val Loss: 14.6272\n",
      "Epoch [248/400] - Train Loss: 13.9892, Val Loss: 14.5853\n",
      "Epoch [249/400] - Train Loss: 13.9802, Val Loss: 14.6118\n",
      "Epoch [250/400] - Train Loss: 13.9688, Val Loss: 14.5929\n",
      "Epoch [251/400] - Train Loss: 13.9578, Val Loss: 14.5924\n",
      "Epoch [252/400] - Train Loss: 13.9512, Val Loss: 14.6153\n",
      "Epoch [253/400] - Train Loss: 13.9476, Val Loss: 14.5913\n",
      "Epoch [254/400] - Train Loss: 13.9424, Val Loss: 14.6204\n",
      "Epoch [255/400] - Train Loss: 13.9350, Val Loss: 14.5938\n",
      "Epoch [256/400] - Train Loss: 13.9245, Val Loss: 14.6073\n",
      "Epoch [257/400] - Train Loss: 13.9144, Val Loss: 14.6021\n",
      "Epoch [258/400] - Train Loss: 13.9058, Val Loss: 14.5998\n",
      "Epoch [259/400] - Train Loss: 13.8989, Val Loss: 14.6149\n",
      "Epoch [260/400] - Train Loss: 13.8920, Val Loss: 14.6011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model.train()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[32m      4\u001b[39m running_train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_data_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating_test_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_data_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating_train_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m        \u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1420\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1421\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1422\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    for user_data_tensor, rating_train_tensor, rating_test_tensor in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(user_data_tensor.to(DEVICE), rating_train_tensor.to(DEVICE)).to(DEVICE)        \n",
    "        loss = criterion(outputs.to(DEVICE), rating_test_tensor.to(DEVICE)) \n",
    "        \n",
    "        loss.backward()                 # Backpropagation\n",
    "        optimizer.step()                # Update model parameters\n",
    "        running_train_loss += loss.item() * rating_test_tensor.size()[1]\n",
    "    \n",
    "    epoch_train_loss = running_train_loss / len(train_dataloader.dataset)\n",
    "\n",
    "    # --- Validation Phase ---32\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for user_data_tensor, rating_train_tensor, rating_test_tensor in val_dataloader:\n",
    "            \n",
    "            outputs = model(user_data_tensor.to(DEVICE), rating_train_tensor.to(DEVICE))        \n",
    "            loss = criterion(outputs, rating_test_tensor.to(DEVICE)) \n",
    "            \n",
    "            running_val_loss += loss.item() * rating_test_tensor.size()[1]\n",
    "    \n",
    "    epoch_val_loss = running_val_loss / len(val_dataloader.dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCH}] - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
